{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from extract_text import TextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of CXRBertModel were not initialized from the model checkpoint at microsoft/BiomedVLP-CXR-BERT-specialized and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/exouser/MedsynBackend/src/extract_text.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.resume_model+\"/pytorch_model.bin\", map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "File already exists: /media/volume/gen-ai-volume/MedSyn/results/text_embed/test_k1.npy\n"
     ]
    }
   ],
   "source": [
    "impressions=\"right pleural effusion and large pancoast tumor in left lung\"\n",
    "output_folder = \"/media/volume/gen-ai-volume/MedSyn/results/text_embed\"  \n",
    "file_name='test_k2.npy'\n",
    "text_extractor = TextExtractor(resume_model=\"/media/volume/gen-ai-volume/MedSyn/models/test_run2\")\n",
    "text_extractor.run(impressions, output_folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/exouser/MedsynBackend/src/stage1.py:1213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=map_location), strict=False)\n",
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading low-res model...\n",
      "File already exists: /media/volume/gen-ai-volume/MedSyn/results/img_64_standard/dont_delete_sample_0.npy\n",
      "Pre-saved noise not found! Generating new fixed noise instead.\n",
      "Low Resolution: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:14<00:00,  3.47it/s]\n",
      "ATTENTION PATH:  /media/volume/gen-ai-volume/MedSyn/results/img_64_standard/test_k1_sample_0_attention.npy\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\n\u001b[1;32m      3\u001b[0m accelerate\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mAcceleratorState\u001b[38;5;241m.\u001b[39m_shared_state\u001b[38;5;241m.\u001b[39mclear() \u001b[38;5;66;03m# dirty hack to reset accelerator state\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_diffusion_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/volume/gen-ai-volume/MedSyn/results/text_embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/volume/gen-ai-volume/MedSyn/results/img_64_standard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/volume/gen-ai-volume/MedSyn/models/stage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnoise_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/volume/gen-ai-volume/MedSyn/results/img_64_standard/saved_noise/test_k1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MedsynBackend/src/stage1.py:1453\u001b[0m, in \u001b[0;36mrun_diffusion_1\u001b[0;34m(input_folder, output_folder, noise_folder, model_folder, num_sample, read_img_flag)\u001b[0m\n\u001b[1;32m   1451\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;66;03m#print(\"training model...\")\u001b[39;00m\n\u001b[0;32m-> 1453\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MedsynBackend/src/stage1.py:1371\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, prob_focus_present, focus_present_mask, log_fn)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     attention_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_folder, file_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attention.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATTENTION PATH: \u001b[39m\u001b[38;5;124m\"\u001b[39m, attention_save_path)\n\u001b[0;32m-> 1371\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(attention_save_path, \u001b[43mall_attention_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile already exists: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(save_path))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "from stage1 import run_diffusion_1\n",
    "import accelerate\n",
    "accelerate.state.AcceleratorState._shared_state.clear() # dirty hack to reset accelerator state\n",
    "run_diffusion_1(input_folder=\"/media/volume/gen-ai-volume/MedSyn/results/text_embed\", \n",
    "                output_folder= \"/media/volume/gen-ai-volume/MedSyn/results/img_64_standard\", \n",
    "                model_folder=\"/media/volume/gen-ai-volume/MedSyn/models/stage1\", \n",
    "                num_sample=1,\n",
    "                noise_folder=\"/media/volume/gen-ai-volume/MedSyn/results/img_64_standard/saved_noise/test_k2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 54307908\n",
      "Map Location: cuda\n",
      "Model path: /media/volume/gen-ai-volume/MedSyn/models/stage2/1000_ckpt/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/MedsynBackend/src/stage2.py:1110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=map_location), strict=False)\n",
      "/home/exouser/MedsynBackend/src/stage2.py:1113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.ema_model.load_state_dict(torch.load(model_path, map_location=map_location), strict=False)\n",
      "Loading dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 174.66it/s]\n",
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/accelerate/checkpointing.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(input_model_file, map_location=map_location)\n",
      "/home/exouser/miniconda3/envs/medsyn-3-8/lib/python3.8/site-packages/accelerate/checkpointing.py:242: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  states = torch.load(input_dir.joinpath(f\"{RNG_STATE_NAME}_{process_index}.pkl\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 videos as gif files at /media/volume/gen-ai-volume/MedSyn/results/img_64_standard\n",
      "loading model...\n",
      "Load accelerator state: /media/volume/gen-ai-volume/MedSyn/models/stage2/1000_ckpt\n",
      "training model...\n",
      "num_samples: 1\n",
      "High Resolution: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [01:26<00:00,  4.11s/it]\n",
      "num_samples: 1\n",
      "File already exists: /media/volume/gen-ai-volume/MedSyn/results/img_256_standard/dont_delete_sample_0.nii.gz\n",
      "num_samples: 1\n",
      "File already exists: /media/volume/gen-ai-volume/MedSyn/results/img_256_standard/20250205145212nopleurale_sample_0.nii.gz\n"
     ]
    }
   ],
   "source": [
    "from stage2 import run_diffusion_2\n",
    "import accelerate\n",
    "accelerate.state.AcceleratorState._shared_state.clear() # dirty hack to reset accelerator state\n",
    "\n",
    "run_diffusion_2(input_folder=\"/media/volume/gen-ai-volume/MedSyn/results/img_64_standard\", \n",
    "                        output_folder=\"/media/volume/gen-ai-volume/MedSyn/results/img_256_standard\", \n",
    "                        model_folder=\"/media/volume/gen-ai-volume/MedSyn/models/stage2\",\n",
    "                        filename=\"test_k1.npy\",\n",
    "                        num_series_exists=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the saliency map generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (1, 4, 64, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "generated_filename = \"test_k2_sample_0.npy\"\n",
    "output_folder = \"/media/volume/gen-ai-volume/MedSyn/results/img_64_standard\"\n",
    "image_path = os.path.join(output_folder, generated_filename)\n",
    "\n",
    "generated_image = np.load(image_path)  # Shape: (C, D, H, W) or (C, num_slices, height, width)\n",
    "\n",
    "print(f\"Loaded image shape: {generated_image.shape}\")  # Check image dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attention map path (modify filename if needed)\n",
    "attention_map_filename = \"20250205145212nopleurale_sample_0_attention.npy\"  # Change if necessary\n",
    "attention_map_path = os.path.join(output_folder, attention_map_filename)\n",
    "\n",
    "# Load the attention maps\n",
    "if os.path.exists(attention_map_path):\n",
    "    attention_maps = np.load(attention_map_path)  # Shape: (num_heads, Slices, Height, Width)\n",
    "    print(f\"Loaded attention map shape: {attention_maps.shape}\")\n",
    "else:\n",
    "    print(\"Attention maps not found! Make sure they were saved.\")\n",
    "    attention_maps = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_attention_on_slice(image_slice, attention_slice, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay attention map on a single DICOM slice.\n",
    "    - image_slice: (H, W) numpy array representing the DICOM slice.\n",
    "    - attention_slice: (H, W) numpy array representing the attention heatmap.\n",
    "    - alpha: blending ratio.\n",
    "    \"\"\"\n",
    "    # Normalize image for visualization\n",
    "    image_slice = (image_slice - np.min(image_slice)) / (np.max(image_slice) - np.min(image_slice))  # Normalize to [0,1]\n",
    "    image_slice = (image_slice * 255).astype(np.uint8)  # Convert to [0,255] range\n",
    "    \n",
    "    # Normalize attention map\n",
    "    attention_slice = (attention_slice - np.min(attention_slice)) / (np.max(attention_slice) - np.min(attention_slice))\n",
    "    attention_slice = (attention_slice * 255).astype(np.uint8)  # Convert to [0,255]\n",
    "    \n",
    "    # Apply colormap to attention map\n",
    "    heatmap = cv2.applyColorMap(attention_slice, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Blend original image and heatmap\n",
    "    overlay = cv2.addWeighted(image_slice, 1 - alpha, heatmap, alpha, 0)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "# Select a few sample slices to visualize\n",
    "num_slices = generated_image.shape[1]  # Number of slices\n",
    "selected_slices = [num_slices // 4, num_slices // 2, 3 * num_slices // 4]  # Select middle slices\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, slice_idx in enumerate(selected_slices):\n",
    "    original_slice = generated_image[0, slice_idx]  # Extract the slice\n",
    "    attention_slice = attention_maps[:, slice_idx].mean(axis=0)  # Average across attention heads\n",
    "    \n",
    "    # Overlay attention\n",
    "    overlay = overlay_attention_on_slice(original_slice, attention_slice)\n",
    "\n",
    "    plt.subplot(1, len(selected_slices), i+1)\n",
    "    plt.imshow(overlay, cmap=\"gray\")\n",
    "    plt.title(f\"Slice {slice_idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsyn-3-8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
